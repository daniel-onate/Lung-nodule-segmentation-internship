{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052922c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 1, 64, 64])\n",
      "Mask batch shape: torch.Size([32, 1, 64, 64])\n",
      "Construct runtime 0.021812915802001953\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import lidcdataset as lidc\n",
    "\n",
    "construct_start = time.time()\n",
    "\n",
    "#constructing the dataset\n",
    "img_dir = \"/Users/daniel/Documents/CSAI/Internship/CODE/data/LUNA16/images/\"\n",
    "mask_dir = \"/Users/daniel/Documents/CSAI/Internship/CODE/data/LUNA16/labels/\"\n",
    "dataset = lidc.LIDCDataset(img_dir, mask_dir)\n",
    "\n",
    "#splitting the dataset\n",
    "train_set, val_set, test_set = random_split(dataset, [0.7, 0.15, 0.15])\n",
    "\n",
    "#data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#test\n",
    "images, masks = next(iter(train_loader))\n",
    "print(\"Image batch shape:\", images.shape)\n",
    "print(\"Mask batch shape:\", masks.shape)\n",
    "\n",
    "construct_end = time.time()\n",
    "print(f\"Construct runtime {construct_end - construct_start}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f017e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import loops\n",
    "import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#setting up training\n",
    "model = models.Unet2D()\n",
    "device = torch.device(\"cpu\")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 300\n",
    "early_stopping = utils.EarlyStopping(patience=15, delta=0.001)\n",
    "\n",
    "#constructing trainer\n",
    "trainer = loops.Trainer(\n",
    "    model=model, \n",
    "    device=device, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    num_epochs=num_epochs, \n",
    "    early_stopping=early_stopping, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader, \n",
    "    save_path=\"Images/loss_test.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2b49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]  Train Loss: 0.6756  Val Loss: 0.7334   Runtime: 1.4831271171569824\n",
      "Epoch [2/300]  Train Loss: 0.5856  Val Loss: 0.7012   Runtime: 1.148231029510498\n",
      "Epoch [3/300]  Train Loss: 0.5576  Val Loss: 0.6607   Runtime: 1.138601303100586\n",
      "Epoch [4/300]  Train Loss: 0.5312  Val Loss: 0.6209   Runtime: 1.1403379440307617\n",
      "Epoch [5/300]  Train Loss: 0.5058  Val Loss: 0.5764   Runtime: 1.2233998775482178\n",
      "Epoch [6/300]  Train Loss: 0.4811  Val Loss: 0.5376   Runtime: 1.1529572010040283\n",
      "Epoch [7/300]  Train Loss: 0.4588  Val Loss: 0.5052   Runtime: 1.198612928390503\n",
      "Epoch [8/300]  Train Loss: 0.4474  Val Loss: 0.4759   Runtime: 1.2010879516601562\n",
      "Epoch [9/300]  Train Loss: 0.4267  Val Loss: 0.4450   Runtime: 1.2791087627410889\n",
      "Epoch [10/300]  Train Loss: 0.4168  Val Loss: 0.4124   Runtime: 1.2720582485198975\n",
      "Epoch [11/300]  Train Loss: 0.4037  Val Loss: 0.3862   Runtime: 1.264894962310791\n",
      "Epoch [12/300]  Train Loss: 0.3935  Val Loss: 0.3616   Runtime: 1.253269910812378\n",
      "Epoch [13/300]  Train Loss: 0.3843  Val Loss: 0.3439   Runtime: 1.3535940647125244\n",
      "Epoch [14/300]  Train Loss: 0.3797  Val Loss: 0.3737   Runtime: 1.252974033355713\n",
      "Epoch [15/300]  Train Loss: 0.3646  Val Loss: 0.3426   Runtime: 1.224289894104004\n",
      "Epoch [16/300]  Train Loss: 0.3544  Val Loss: 0.3581   Runtime: 1.2307970523834229\n",
      "Epoch [17/300]  Train Loss: 0.3504  Val Loss: 0.3591   Runtime: 1.200727939605713\n",
      "Epoch [18/300]  Train Loss: 0.3410  Val Loss: 0.3234   Runtime: 1.4262049198150635\n",
      "Epoch [19/300]  Train Loss: 0.3334  Val Loss: 0.3121   Runtime: 1.3416368961334229\n",
      "Epoch [20/300]  Train Loss: 0.3301  Val Loss: 0.3039   Runtime: 1.2849442958831787\n",
      "Epoch [21/300]  Train Loss: 0.3229  Val Loss: 0.2985   Runtime: 1.2144980430603027\n",
      "Epoch [22/300]  Train Loss: 0.3212  Val Loss: 0.3008   Runtime: 1.2268590927124023\n",
      "Epoch [23/300]  Train Loss: 0.3005  Val Loss: 0.3111   Runtime: 1.3374128341674805\n",
      "Epoch [24/300]  Train Loss: 0.3040  Val Loss: 0.2976   Runtime: 1.316612958908081\n",
      "Epoch [25/300]  Train Loss: 0.2915  Val Loss: 0.2797   Runtime: 1.3770358562469482\n",
      "Epoch [26/300]  Train Loss: 0.2837  Val Loss: 0.2758   Runtime: 1.2448620796203613\n",
      "Epoch [27/300]  Train Loss: 0.2787  Val Loss: 0.2688   Runtime: 1.1600420475006104\n",
      "Epoch [28/300]  Train Loss: 0.2805  Val Loss: 0.2612   Runtime: 1.1159300804138184\n",
      "Epoch [29/300]  Train Loss: 0.2671  Val Loss: 0.2615   Runtime: 1.1377558708190918\n",
      "Epoch [30/300]  Train Loss: 0.2682  Val Loss: 0.2587   Runtime: 1.161634922027588\n",
      "Epoch [31/300]  Train Loss: 0.2546  Val Loss: 0.2485   Runtime: 1.1504559516906738\n",
      "Epoch [32/300]  Train Loss: 0.2485  Val Loss: 0.2383   Runtime: 1.1510260105133057\n",
      "Epoch [33/300]  Train Loss: 0.2481  Val Loss: 0.2324   Runtime: 1.2815651893615723\n",
      "Epoch [34/300]  Train Loss: 0.2413  Val Loss: 0.2305   Runtime: 1.3644988536834717\n",
      "Epoch [35/300]  Train Loss: 0.2351  Val Loss: 0.2298   Runtime: 1.3772289752960205\n",
      "Epoch [36/300]  Train Loss: 0.2341  Val Loss: 0.2266   Runtime: 1.4222381114959717\n",
      "Epoch [37/300]  Train Loss: 0.2296  Val Loss: 0.2200   Runtime: 1.5059833526611328\n",
      "Epoch [38/300]  Train Loss: 0.2223  Val Loss: 0.2138   Runtime: 1.3674147129058838\n",
      "Epoch [39/300]  Train Loss: 0.2197  Val Loss: 0.2061   Runtime: 1.363206148147583\n",
      "Epoch [40/300]  Train Loss: 0.2121  Val Loss: 0.2030   Runtime: 1.4547481536865234\n",
      "Epoch [41/300]  Train Loss: 0.2072  Val Loss: 0.1993   Runtime: 1.4983389377593994\n",
      "Epoch [42/300]  Train Loss: 0.2075  Val Loss: 0.1945   Runtime: 1.2430157661437988\n",
      "Epoch [43/300]  Train Loss: 0.2123  Val Loss: 0.1914   Runtime: 1.2344310283660889\n",
      "Epoch [44/300]  Train Loss: 0.1975  Val Loss: 0.1937   Runtime: 1.1858596801757812\n",
      "Epoch [45/300]  Train Loss: 0.1894  Val Loss: 0.1900   Runtime: 1.2865970134735107\n",
      "Epoch [46/300]  Train Loss: 0.1844  Val Loss: 0.1864   Runtime: 1.177520990371704\n",
      "Epoch [47/300]  Train Loss: 0.1845  Val Loss: 0.1842   Runtime: 1.140308141708374\n",
      "Epoch [48/300]  Train Loss: 0.1864  Val Loss: 0.1814   Runtime: 1.1472649574279785\n",
      "Epoch [49/300]  Train Loss: 0.1803  Val Loss: 0.1769   Runtime: 1.1302599906921387\n",
      "Epoch [50/300]  Train Loss: 0.1738  Val Loss: 0.1726   Runtime: 1.144402027130127\n",
      "Epoch [51/300]  Train Loss: 0.1745  Val Loss: 0.1682   Runtime: 1.1275768280029297\n",
      "Epoch [52/300]  Train Loss: 0.1684  Val Loss: 0.1628   Runtime: 1.1383490562438965\n",
      "Epoch [53/300]  Train Loss: 0.1656  Val Loss: 0.1566   Runtime: 1.154567003250122\n",
      "Epoch [54/300]  Train Loss: 0.1602  Val Loss: 0.1533   Runtime: 1.1582341194152832\n",
      "Epoch [55/300]  Train Loss: 0.1593  Val Loss: 0.1516   Runtime: 1.1388659477233887\n",
      "Epoch [56/300]  Train Loss: 0.1542  Val Loss: 0.1501   Runtime: 1.2008440494537354\n",
      "Epoch [57/300]  Train Loss: 0.1579  Val Loss: 0.1605   Runtime: 1.3156330585479736\n",
      "Epoch [58/300]  Train Loss: 0.1526  Val Loss: 0.1510   Runtime: 1.323188304901123\n",
      "Epoch [59/300]  Train Loss: 0.1491  Val Loss: 0.1522   Runtime: 1.1182861328125\n",
      "Epoch [60/300]  Train Loss: 0.1453  Val Loss: 0.1469   Runtime: 1.2487308979034424\n",
      "Epoch [61/300]  Train Loss: 0.1465  Val Loss: 0.1401   Runtime: 1.188791036605835\n",
      "Epoch [62/300]  Train Loss: 0.1422  Val Loss: 0.1377   Runtime: 1.1571271419525146\n",
      "Epoch [63/300]  Train Loss: 0.1366  Val Loss: 0.1318   Runtime: 1.2022929191589355\n",
      "Epoch [64/300]  Train Loss: 0.1341  Val Loss: 0.1321   Runtime: 1.151663064956665\n",
      "Epoch [65/300]  Train Loss: 0.1370  Val Loss: 0.1309   Runtime: 1.3097360134124756\n",
      "Epoch [66/300]  Train Loss: 0.1301  Val Loss: 0.1277   Runtime: 1.2226603031158447\n",
      "Epoch [67/300]  Train Loss: 0.1322  Val Loss: 0.1272   Runtime: 1.176682949066162\n",
      "Epoch [68/300]  Train Loss: 0.1275  Val Loss: 0.1275   Runtime: 1.189497947692871\n",
      "Epoch [69/300]  Train Loss: 0.1246  Val Loss: 0.1274   Runtime: 1.1768767833709717\n",
      "Epoch [70/300]  Train Loss: 0.1218  Val Loss: 0.1242   Runtime: 1.1898958683013916\n",
      "Epoch [71/300]  Train Loss: 0.1170  Val Loss: 0.1211   Runtime: 1.173466682434082\n",
      "Epoch [72/300]  Train Loss: 0.1194  Val Loss: 0.1179   Runtime: 1.253605842590332\n",
      "Epoch [73/300]  Train Loss: 0.1181  Val Loss: 0.1137   Runtime: 1.171187162399292\n",
      "Epoch [74/300]  Train Loss: 0.1190  Val Loss: 0.1118   Runtime: 1.1973850727081299\n",
      "Epoch [75/300]  Train Loss: 0.1155  Val Loss: 0.1090   Runtime: 1.2413101196289062\n",
      "Epoch [76/300]  Train Loss: 0.1128  Val Loss: 0.1082   Runtime: 1.1413447856903076\n",
      "Epoch [77/300]  Train Loss: 0.1148  Val Loss: 0.1074   Runtime: 1.1385090351104736\n",
      "Epoch [78/300]  Train Loss: 0.1090  Val Loss: 0.1073   Runtime: 1.1511907577514648\n",
      "Epoch [79/300]  Train Loss: 0.1036  Val Loss: 0.1041   Runtime: 1.2222940921783447\n",
      "Epoch [80/300]  Train Loss: 0.1059  Val Loss: 0.0999   Runtime: 1.153205156326294\n",
      "Epoch [81/300]  Train Loss: 0.1047  Val Loss: 0.0990   Runtime: 1.1855380535125732\n",
      "Epoch [82/300]  Train Loss: 0.1048  Val Loss: 0.0975   Runtime: 1.2140321731567383\n",
      "Epoch [83/300]  Train Loss: 0.1009  Val Loss: 0.0939   Runtime: 1.1331491470336914\n",
      "Epoch [84/300]  Train Loss: 0.1012  Val Loss: 0.0948   Runtime: 1.0979669094085693\n",
      "Epoch [85/300]  Train Loss: 0.1024  Val Loss: 0.0944   Runtime: 1.1110360622406006\n",
      "Epoch [86/300]  Train Loss: 0.0947  Val Loss: 0.0939   Runtime: 1.2070930004119873\n",
      "Epoch [87/300]  Train Loss: 0.0949  Val Loss: 0.0913   Runtime: 1.1809427738189697\n",
      "Epoch [88/300]  Train Loss: 0.1010  Val Loss: 0.0904   Runtime: 1.156285047531128\n",
      "Epoch [89/300]  Train Loss: 0.0926  Val Loss: 0.0902   Runtime: 1.1593661308288574\n",
      "Epoch [90/300]  Train Loss: 0.0917  Val Loss: 0.0880   Runtime: 1.1570301055908203\n",
      "Epoch [91/300]  Train Loss: 0.0893  Val Loss: 0.0884   Runtime: 1.149972915649414\n",
      "Epoch [92/300]  Train Loss: 0.0921  Val Loss: 0.0844   Runtime: 1.1353600025177002\n",
      "Epoch [93/300]  Train Loss: 0.0845  Val Loss: 0.0851   Runtime: 1.1125760078430176\n",
      "Epoch [94/300]  Train Loss: 0.0874  Val Loss: 0.0812   Runtime: 1.1112959384918213\n",
      "Epoch [95/300]  Train Loss: 0.0816  Val Loss: 0.0810   Runtime: 1.123924970626831\n",
      "Epoch [96/300]  Train Loss: 0.0833  Val Loss: 0.0792   Runtime: 1.167875051498413\n",
      "Epoch [97/300]  Train Loss: 0.0822  Val Loss: 0.0793   Runtime: 1.439758062362671\n",
      "Epoch [98/300]  Train Loss: 0.0834  Val Loss: 0.0790   Runtime: 1.2806661128997803\n",
      "Epoch [99/300]  Train Loss: 0.0777  Val Loss: 0.0798   Runtime: 1.1542809009552002\n",
      "Epoch [100/300]  Train Loss: 0.0793  Val Loss: 0.0767   Runtime: 1.1219351291656494\n",
      "Epoch [101/300]  Train Loss: 0.0723  Val Loss: 0.0766   Runtime: 1.1511340141296387\n",
      "Epoch [102/300]  Train Loss: 0.0764  Val Loss: 0.0736   Runtime: 1.1314339637756348\n",
      "Epoch [103/300]  Train Loss: 0.0756  Val Loss: 0.0731   Runtime: 1.1815299987792969\n",
      "Epoch [104/300]  Train Loss: 0.0802  Val Loss: 0.0743   Runtime: 1.1423900127410889\n",
      "Epoch [105/300]  Train Loss: 0.0698  Val Loss: 0.0718   Runtime: 1.1489591598510742\n",
      "Epoch [106/300]  Train Loss: 0.0754  Val Loss: 0.0705   Runtime: 1.1480090618133545\n",
      "Epoch [107/300]  Train Loss: 0.0707  Val Loss: 0.0692   Runtime: 1.1354060173034668\n",
      "Epoch [108/300]  Train Loss: 0.0709  Val Loss: 0.0689   Runtime: 1.1455788612365723\n",
      "Epoch [109/300]  Train Loss: 0.0720  Val Loss: 0.0668   Runtime: 1.172489881515503\n",
      "Epoch [110/300]  Train Loss: 0.0655  Val Loss: 0.0663   Runtime: 1.1539220809936523\n",
      "Epoch [111/300]  Train Loss: 0.0684  Val Loss: 0.0665   Runtime: 1.1298980712890625\n",
      "Epoch [112/300]  Train Loss: 0.0647  Val Loss: 0.0648   Runtime: 1.1590828895568848\n",
      "Epoch [113/300]  Train Loss: 0.0679  Val Loss: 0.0650   Runtime: 1.15460205078125\n",
      "Epoch [114/300]  Train Loss: 0.0641  Val Loss: 0.0633   Runtime: 1.1376776695251465\n",
      "Epoch [115/300]  Train Loss: 0.0650  Val Loss: 0.0615   Runtime: 1.1959030628204346\n",
      "Epoch [116/300]  Train Loss: 0.0618  Val Loss: 0.0609   Runtime: 1.1634011268615723\n",
      "Epoch [117/300]  Train Loss: 0.0635  Val Loss: 0.0614   Runtime: 1.146803855895996\n",
      "Epoch [118/300]  Train Loss: 0.0610  Val Loss: 0.0616   Runtime: 1.148380994796753\n",
      "Epoch [119/300]  Train Loss: 0.0633  Val Loss: 0.0600   Runtime: 1.146306037902832\n",
      "Epoch [120/300]  Train Loss: 0.0591  Val Loss: 0.0595   Runtime: 1.1724648475646973\n",
      "Epoch [121/300]  Train Loss: 0.0583  Val Loss: 0.0589   Runtime: 1.1895759105682373\n",
      "Epoch [122/300]  Train Loss: 0.0596  Val Loss: 0.0583   Runtime: 1.2002310752868652\n",
      "Epoch [123/300]  Train Loss: 0.0573  Val Loss: 0.0577   Runtime: 1.2723948955535889\n",
      "Epoch [124/300]  Train Loss: 0.0571  Val Loss: 0.0575   Runtime: 1.227973222732544\n",
      "Epoch [125/300]  Train Loss: 0.0577  Val Loss: 0.0577   Runtime: 1.1409950256347656\n",
      "Epoch [126/300]  Train Loss: 0.0564  Val Loss: 0.0569   Runtime: 1.2061781883239746\n",
      "Epoch [127/300]  Train Loss: 0.0522  Val Loss: 0.0559   Runtime: 1.4456210136413574\n",
      "Epoch [128/300]  Train Loss: 0.0571  Val Loss: 0.0550   Runtime: 1.3253118991851807\n",
      "Epoch [129/300]  Train Loss: 0.0535  Val Loss: 0.0547   Runtime: 1.2321138381958008\n",
      "Epoch [130/300]  Train Loss: 0.0558  Val Loss: 0.0536   Runtime: 1.2079071998596191\n",
      "Epoch [131/300]  Train Loss: 0.0552  Val Loss: 0.0543   Runtime: 1.1458747386932373\n",
      "Epoch [132/300]  Train Loss: 0.0552  Val Loss: 0.0529   Runtime: 1.221437931060791\n",
      "Epoch [133/300]  Train Loss: 0.0531  Val Loss: 0.0518   Runtime: 1.1215848922729492\n",
      "Epoch [134/300]  Train Loss: 0.0526  Val Loss: 0.0526   Runtime: 1.1393871307373047\n",
      "Epoch [135/300]  Train Loss: 0.0526  Val Loss: 0.0524   Runtime: 1.2142746448516846\n",
      "Epoch [136/300]  Train Loss: 0.0516  Val Loss: 0.0515   Runtime: 1.149846076965332\n",
      "Epoch [137/300]  Train Loss: 0.0498  Val Loss: 0.0503   Runtime: 1.1958239078521729\n",
      "Epoch [138/300]  Train Loss: 0.0511  Val Loss: 0.0508   Runtime: 1.1429469585418701\n",
      "Epoch [139/300]  Train Loss: 0.0490  Val Loss: 0.0501   Runtime: 1.1550090312957764\n",
      "Epoch [140/300]  Train Loss: 0.0513  Val Loss: 0.0488   Runtime: 1.132500171661377\n",
      "Epoch [141/300]  Train Loss: 0.0523  Val Loss: 0.0481   Runtime: 1.1597650051116943\n",
      "Epoch [142/300]  Train Loss: 0.0506  Val Loss: 0.0482   Runtime: 1.1346638202667236\n",
      "Epoch [143/300]  Train Loss: 0.0456  Val Loss: 0.0483   Runtime: 1.1391921043395996\n",
      "Epoch [144/300]  Train Loss: 0.0453  Val Loss: 0.0465   Runtime: 1.1396377086639404\n",
      "Epoch [145/300]  Train Loss: 0.0473  Val Loss: 0.0459   Runtime: 1.1461031436920166\n",
      "Epoch [146/300]  Train Loss: 0.0444  Val Loss: 0.0452   Runtime: 1.1446831226348877\n",
      "Epoch [147/300]  Train Loss: 0.0452  Val Loss: 0.0456   Runtime: 1.136197805404663\n",
      "Epoch [148/300]  Train Loss: 0.0460  Val Loss: 0.0448   Runtime: 1.1433279514312744\n",
      "Epoch [149/300]  Train Loss: 0.0476  Val Loss: 0.0454   Runtime: 1.1296801567077637\n",
      "Epoch [150/300]  Train Loss: 0.0486  Val Loss: 0.0452   Runtime: 1.255256175994873\n",
      "Epoch [151/300]  Train Loss: 0.0441  Val Loss: 0.0438   Runtime: 1.3286097049713135\n",
      "Epoch [152/300]  Train Loss: 0.0460  Val Loss: 0.0442   Runtime: 1.209651231765747\n",
      "Epoch [153/300]  Train Loss: 0.0433  Val Loss: 0.0430   Runtime: 1.2381477355957031\n",
      "Epoch [154/300]  Train Loss: 0.0474  Val Loss: 0.0440   Runtime: 1.1479780673980713\n",
      "Epoch [155/300]  Train Loss: 0.0454  Val Loss: 0.0440   Runtime: 1.2087838649749756\n",
      "Epoch [156/300]  Train Loss: 0.0408  Val Loss: 0.0444   Runtime: 1.2115399837493896\n",
      "Epoch [157/300]  Train Loss: 0.0413  Val Loss: 0.0442   Runtime: 1.153414249420166\n",
      "Epoch [158/300]  Train Loss: 0.0415  Val Loss: 0.0437   Runtime: 1.1539256572723389\n",
      "Epoch [159/300]  Train Loss: 0.0410  Val Loss: 0.0424   Runtime: 1.1299989223480225\n",
      "Epoch [160/300]  Train Loss: 0.0416  Val Loss: 0.0416   Runtime: 1.1237668991088867\n",
      "Epoch [161/300]  Train Loss: 0.0393  Val Loss: 0.0415   Runtime: 1.1394128799438477\n",
      "Epoch [162/300]  Train Loss: 0.0409  Val Loss: 0.0404   Runtime: 1.1252570152282715\n",
      "Epoch [163/300]  Train Loss: 0.0396  Val Loss: 0.0404   Runtime: 1.1334221363067627\n",
      "Epoch [164/300]  Train Loss: 0.0404  Val Loss: 0.0412   Runtime: 1.142906904220581\n",
      "Epoch [165/300]  Train Loss: 0.0367  Val Loss: 0.0405   Runtime: 1.1191370487213135\n",
      "Epoch [166/300]  Train Loss: 0.0402  Val Loss: 0.0388   Runtime: 1.1651058197021484\n",
      "Epoch [167/300]  Train Loss: 0.0393  Val Loss: 0.0392   Runtime: 1.3307387828826904\n",
      "Epoch [168/300]  Train Loss: 0.0364  Val Loss: 0.0397   Runtime: 1.2560100555419922\n",
      "Epoch [169/300]  Train Loss: 0.0387  Val Loss: 0.0386   Runtime: 1.1845698356628418\n",
      "Epoch [170/300]  Train Loss: 0.0372  Val Loss: 0.0390   Runtime: 1.1272010803222656\n",
      "Epoch [171/300]  Train Loss: 0.0404  Val Loss: 0.0391   Runtime: 1.1274051666259766\n",
      "Epoch [172/300]  Train Loss: 0.0379  Val Loss: 0.0393   Runtime: 1.127284049987793\n",
      "Epoch [173/300]  Train Loss: 0.0378  Val Loss: 0.0380   Runtime: 1.1187927722930908\n",
      "Epoch [174/300]  Train Loss: 0.0379  Val Loss: 0.0384   Runtime: 1.1373519897460938\n",
      "Epoch [175/300]  Train Loss: 0.0391  Val Loss: 0.0379   Runtime: 1.2248530387878418\n",
      "Epoch [176/300]  Train Loss: 0.0343  Val Loss: 0.0362   Runtime: 1.1849501132965088\n",
      "Epoch [177/300]  Train Loss: 0.0383  Val Loss: 0.0368   Runtime: 1.1776461601257324\n",
      "Epoch [178/300]  Train Loss: 0.0372  Val Loss: 0.0374   Runtime: 1.3514440059661865\n",
      "Epoch [179/300]  Train Loss: 0.0360  Val Loss: 0.0365   Runtime: 1.1661980152130127\n",
      "Epoch [180/300]  Train Loss: 0.0340  Val Loss: 0.0370   Runtime: 1.154310703277588\n",
      "Epoch [181/300]  Train Loss: 0.0342  Val Loss: 0.0358   Runtime: 1.1843950748443604\n",
      "Epoch [182/300]  Train Loss: 0.0352  Val Loss: 0.0357   Runtime: 1.526228904724121\n",
      "Epoch [183/300]  Train Loss: 0.0317  Val Loss: 0.0364   Runtime: 1.1707229614257812\n",
      "Epoch [184/300]  Train Loss: 0.0333  Val Loss: 0.0355   Runtime: 1.1327118873596191\n",
      "Epoch [185/300]  Train Loss: 0.0361  Val Loss: 0.0347   Runtime: 1.1916680335998535\n",
      "Epoch [186/300]  Train Loss: 0.0366  Val Loss: 0.0341   Runtime: 1.1266968250274658\n",
      "Epoch [187/300]  Train Loss: 0.0338  Val Loss: 0.0349   Runtime: 1.1758277416229248\n",
      "Epoch [188/300]  Train Loss: 0.0350  Val Loss: 0.0348   Runtime: 1.5101091861724854\n",
      "Epoch [189/300]  Train Loss: 0.0346  Val Loss: 0.0347   Runtime: 1.2766029834747314\n",
      "Epoch [190/300]  Train Loss: 0.0337  Val Loss: 0.0351   Runtime: 1.4188990592956543\n",
      "Epoch [191/300]  Train Loss: 0.0356  Val Loss: 0.0339   Runtime: 1.905055046081543\n",
      "Epoch [192/300]  Train Loss: 0.0308  Val Loss: 0.0346   Runtime: 1.7320001125335693\n",
      "Epoch [193/300]  Train Loss: 0.0354  Val Loss: 0.0337   Runtime: 1.6110830307006836\n",
      "Epoch [194/300]  Train Loss: 0.0334  Val Loss: 0.0331   Runtime: 1.8876490592956543\n",
      "Epoch [195/300]  Train Loss: 0.0324  Val Loss: 0.0339   Runtime: 1.570518970489502\n",
      "Epoch [196/300]  Train Loss: 0.0337  Val Loss: 0.0339   Runtime: 1.4545447826385498\n",
      "Epoch [197/300]  Train Loss: 0.0294  Val Loss: 0.0329   Runtime: 1.289762020111084\n",
      "Epoch [198/300]  Train Loss: 0.0303  Val Loss: 0.0330   Runtime: 1.173971176147461\n",
      "Epoch [199/300]  Train Loss: 0.0320  Val Loss: 0.0335   Runtime: 1.3624987602233887\n",
      "Epoch [200/300]  Train Loss: 0.0298  Val Loss: 0.0336   Runtime: 1.4791560173034668\n",
      "Epoch [201/300]  Train Loss: 0.0305  Val Loss: 0.0333   Runtime: 1.656003713607788\n",
      "Epoch [202/300]  Train Loss: 0.0339  Val Loss: 0.0337   Runtime: 1.3136651515960693\n",
      "Epoch [203/300]  Train Loss: 0.0305  Val Loss: 0.0328   Runtime: 1.1556119918823242\n",
      "Epoch [204/300]  Train Loss: 0.0319  Val Loss: 0.0328   Runtime: 1.1491670608520508\n",
      "Epoch [205/300]  Train Loss: 0.0309  Val Loss: 0.0331   Runtime: 1.1434259414672852\n",
      "Epoch [206/300]  Train Loss: 0.0292  Val Loss: 0.0319   Runtime: 1.1294310092926025\n",
      "Epoch [207/300]  Train Loss: 0.0300  Val Loss: 0.0318   Runtime: 1.1569390296936035\n",
      "Epoch [208/300]  Train Loss: 0.0287  Val Loss: 0.0321   Runtime: 1.1594130992889404\n",
      "Epoch [209/300]  Train Loss: 0.0300  Val Loss: 0.0313   Runtime: 1.1550133228302002\n",
      "Epoch [210/300]  Train Loss: 0.0294  Val Loss: 0.0314   Runtime: 1.1711366176605225\n",
      "Epoch [211/300]  Train Loss: 0.0286  Val Loss: 0.0328   Runtime: 1.1781351566314697\n",
      "Epoch [212/300]  Train Loss: 0.0298  Val Loss: 0.0318   Runtime: 1.140321969985962\n",
      "Epoch [213/300]  Train Loss: 0.0302  Val Loss: 0.0316   Runtime: 1.1890909671783447\n",
      "Epoch [214/300]  Train Loss: 0.0261  Val Loss: 0.0325   Runtime: 1.1691961288452148\n",
      "Epoch [215/300]  Train Loss: 0.0272  Val Loss: 0.0319   Runtime: 1.146751880645752\n",
      "Epoch [216/300]  Train Loss: 0.0258  Val Loss: 0.0330   Runtime: 1.1370129585266113\n",
      "Epoch [217/300]  Train Loss: 0.0271  Val Loss: 0.0325   Runtime: 1.1525688171386719\n",
      "Epoch [218/300]  Train Loss: 0.0259  Val Loss: 0.0322   Runtime: 1.3398432731628418\n",
      "Epoch [219/300]  Train Loss: 0.0279  Val Loss: 0.0312   Runtime: 1.174548864364624\n",
      "Epoch [220/300]  Train Loss: 0.0286  Val Loss: 0.0307   Runtime: 1.218456745147705\n",
      "Epoch [221/300]  Train Loss: 0.0272  Val Loss: 0.0299   Runtime: 1.2124049663543701\n",
      "Epoch [222/300]  Train Loss: 0.0274  Val Loss: 0.0294   Runtime: 1.477553129196167\n",
      "Epoch [223/300]  Train Loss: 0.0281  Val Loss: 0.0304   Runtime: 1.2390999794006348\n",
      "Epoch [224/300]  Train Loss: 0.0295  Val Loss: 0.0299   Runtime: 1.173008918762207\n",
      "Epoch [225/300]  Train Loss: 0.0277  Val Loss: 0.0309   Runtime: 1.2133188247680664\n",
      "Epoch [226/300]  Train Loss: 0.0261  Val Loss: 0.0319   Runtime: 1.144967794418335\n",
      "Epoch [227/300]  Train Loss: 0.0239  Val Loss: 0.0309   Runtime: 1.1358890533447266\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3897ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test runtime 0.049492835998535156\n",
      "Mean dice coefficient on test set: 0.8916\n",
      "Mean loss on test set: 0.0290\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a86e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[33;03m'''# Example usage:\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m# After testing loop\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03mif __name__ == \"__main__\":\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m \u001b[33;03m        save_dir='test_predictions'\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03m    )'''\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Or visualize just one example\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[43mmodel\u001b[49m.eval()\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    142\u001b[39m     images, masks = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = trainer.get_model()\n",
    "\n",
    "def visualize_prediction(image, mask, prediction, threshold=0.5, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize segmentation prediction with color-coded overlay.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image tensor (1, H, W) or (H, W)\n",
    "        mask: Ground truth mask tensor (1, H, W) or (H, W)\n",
    "        prediction: Model prediction tensor (1, H, W) or (H, W), values 0-1\n",
    "        threshold: Threshold for binarizing predictions (default 0.5)\n",
    "        save_path: Path to save the figure (optional)\n",
    "    \n",
    "    Color coding:\n",
    "        - White: Intersection (True Positives)\n",
    "        - Blue: Mask only (False Negatives)\n",
    "        - Red: Prediction only (False Positives)\n",
    "        - Black: Background (True Negatives)\n",
    "    \"\"\"\n",
    "    # Convert to numpy and remove channel dimension if present\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().squeeze().numpy()\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.cpu().squeeze().numpy()\n",
    "    if isinstance(prediction, torch.Tensor):\n",
    "        prediction = prediction.cpu().squeeze().numpy()\n",
    "    \n",
    "    # Binarize prediction\n",
    "    pred_binary = (prediction > threshold).astype(np.float32)\n",
    "    mask_binary = mask.astype(np.float32)\n",
    "    \n",
    "    # Calculate intersection and differences\n",
    "    intersection = (pred_binary * mask_binary)  # Both 1 (True Positive)\n",
    "    mask_only = mask_binary - intersection      # Mask=1, Pred=0 (False Negative)\n",
    "    pred_only = pred_binary - intersection      # Pred=1, Mask=0 (False Positive)\n",
    "    \n",
    "    # Create RGB overlay\n",
    "    overlay = np.zeros((*image.shape, 3))\n",
    "    \n",
    "    # White for intersection (TP)\n",
    "    overlay[intersection == 1] = [1, 1, 1]\n",
    "    \n",
    "    # Blue for mask only (FN)\n",
    "    overlay[mask_only == 1] = [0, 0, 1]\n",
    "    \n",
    "    # Red for prediction only (FP)\n",
    "    overlay[pred_only == 1] = [1, 0, 0]\n",
    "    \n",
    "    # Create figure with 4 subplots\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Ground truth mask\n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[2].imshow(prediction, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2].set_title(f'Prediction (raw)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Color-coded overlay\n",
    "    axes[3].imshow(overlay)\n",
    "    axes[3].set_title('Overlay\\n(White=TP, Blue=FN, Red=FP)')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_multiple_predictions(model, test_loader, device, num_samples=5, \n",
    "                              threshold=0.5, save_dir='predictions'):\n",
    "    \"\"\"\n",
    "    Save multiple prediction visualizations.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_loader: Test data loader\n",
    "        device: Device (cuda/cpu)\n",
    "        num_samples: Number of samples to visualize\n",
    "        threshold: Threshold for binarizing predictions\n",
    "        save_dir: Directory to save images\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Visualize samples from this batch\n",
    "            batch_size = images.size(0)\n",
    "            for i in range(min(batch_size, num_samples - count)):\n",
    "                save_path = os.path.join(save_dir, f'prediction_{count+1}.png')\n",
    "                visualize_prediction(\n",
    "                    images[i],\n",
    "                    masks[i],\n",
    "                    outputs[i],\n",
    "                    threshold=threshold,\n",
    "                    save_path=save_path\n",
    "                )\n",
    "                count += 1\n",
    "                \n",
    "                if count >= num_samples:\n",
    "                    return\n",
    "    \n",
    "    print(f\"Saved {count} visualizations to {save_dir}/\")\n",
    "\n",
    "\n",
    "'''# Example usage:\n",
    "# After testing loop\n",
    "if __name__ == \"__main__\":\n",
    "    # Save 5 example predictions\n",
    "    save_multiple_predictions(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        num_samples=5,\n",
    "        threshold=0.5,\n",
    "        save_dir='test_predictions'\n",
    "    )'''\n",
    "    \n",
    "# Or visualize just one example\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, masks = next(iter(test_loader))\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # Visualize first image in batch\n",
    "    visualize_prediction(\n",
    "        images[0],\n",
    "        masks[0],\n",
    "        outputs[0],\n",
    "        threshold=0.5,\n",
    "        save_path='Images/single_prediction.png'\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
