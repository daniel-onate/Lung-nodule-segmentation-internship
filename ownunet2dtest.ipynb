{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052922c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 1, 64, 64])\n",
      "Mask batch shape: torch.Size([32, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class LIDCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, mask_dir):\n",
    "\n",
    "        self.image_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted(os.listdir(self.image_dir))\n",
    "        self.mask_files = sorted(os.listdir(self.mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Load the numpy arrays\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        image = np.load(img_path).astype(np.float32)\n",
    "        mask = np.load(mask_path).astype(np.float32)\n",
    "\n",
    "        # Normalize image and mask between 0 and 1\n",
    "        image = (image - image.min()) / (image.max() - image.min() + 1e-8)\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-8)\n",
    "\n",
    "        # Add channel dimension (1, H, W)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        image = torch.from_numpy(image)\n",
    "        mask = torch.from_numpy(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "#constructing the dataset\n",
    "img_dir = \"/Users/daniel/Documents/CSAI/Internship/CODE/data/LUNA16/images/\"\n",
    "mask_dir = \"/Users/daniel/Documents/CSAI/Internship/CODE/data/LUNA16/labels/\"\n",
    "dataset = LIDCDataset(img_dir, mask_dir)\n",
    "\n",
    "#splitting the dataset\n",
    "train_set, val_set, test_set = random_split(dataset, [0.7, 0.15, 0.15])\n",
    "\n",
    "#data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#test\n",
    "images, masks = next(iter(train_loader))\n",
    "print(\"Image batch shape:\", images.shape)\n",
    "print(\"Mask batch shape:\", masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0404a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#variables\n",
    "conv_kernel_size = 3\n",
    "conv_padding = 1\n",
    "upconv_kernel_size = 2\n",
    "upconv_stride = 2\n",
    "pool_kernel_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #encoder\n",
    "        self.conv1_1 = nn.Conv2d(1, 64, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "\n",
    "        self.drop1 = nn.Dropout2d(0.1)\n",
    "        self.drop2 = nn.Dropout2d(0.1)\n",
    "        self.drop3 = nn.Dropout2d(0.1)\n",
    "        self.drop4 = nn.Dropout2d(0.1)\n",
    "        self.drop5 = nn.Dropout2d(0.1)\n",
    "        self.drop6 = nn.Dropout2d(0.1)\n",
    "        self.drop7 = nn.Dropout2d(0.1)\n",
    "        self.drop8 = nn.Dropout2d(0.1)\n",
    "        self.drop9 = nn.Dropout2d(0.1)\n",
    "\n",
    "        #bottleneck\n",
    "        self.conv5_1 = nn.Conv2d(512, 1024, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv5_2 = nn.Conv2d(1024, 1024, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "\n",
    "        #decoder\n",
    "        self.conv6_1 = nn.Conv2d(1024, 512, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv6_2 = nn.Conv2d(512, 512, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv7_1 = nn.Conv2d(512, 256, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv7_2 = nn.Conv2d(256, 256, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv8_1 = nn.Conv2d(256, 128, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv8_2 = nn.Conv2d(128, 128, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv9_1 = nn.Conv2d(128, 64, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv9_2 = nn.Conv2d(64, 64, kernel_size=conv_kernel_size, padding=conv_padding)\n",
    "        self.conv10 = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "        self.upconv6 = nn.ConvTranspose2d(1024, 512, kernel_size=upconv_kernel_size, stride=upconv_stride)\n",
    "        self.upconv7 = nn.ConvTranspose2d(512, 256, kernel_size=upconv_kernel_size, stride=upconv_stride)\n",
    "        self.upconv8 = nn.ConvTranspose2d(256, 128, kernel_size=upconv_kernel_size, stride=upconv_stride)\n",
    "        self.upconv9 = nn.ConvTranspose2d(128, 64, kernel_size=upconv_kernel_size, stride=upconv_stride)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #encoder\n",
    "        c1 = F.relu(self.conv1_1(x))\n",
    "        c1 = self.drop1(c1)\n",
    "        c1 = F.relu(self.conv1_2(c1))\n",
    "        p1 = self.pool(c1)\n",
    "        c2 = F.relu(self.conv2_1(p1))\n",
    "        c2 = self.drop2(c2)\n",
    "        c2 = F.relu(self.conv2_2(c2))\n",
    "        p2 = self.pool(c2)  \n",
    "        c3 = F.relu(self.conv3_1(p2))\n",
    "        c3 = self.drop3(c3)\n",
    "        c3 = F.relu(self.conv3_2(c3))\n",
    "        p3 = self.pool(c3)  \n",
    "        c4 = F.relu(self.conv4_1(p3))\n",
    "        c4 = self.drop4(c4)\n",
    "        c4 = F.relu(self.conv4_2(c4))\n",
    "        p4 = self.pool(c4)\n",
    "\n",
    "        #bottleneck\n",
    "        c5 = F.relu(self.conv5_1(p4))\n",
    "        c5 = self.drop5(c5)\n",
    "        c5 = F.relu(self.conv5_2(c5))\n",
    "\n",
    "        #decoder\n",
    "        u6 = torch.cat((self.upconv6(c5), c4), dim=1)\n",
    "        c6 = F.relu(self.conv6_1(u6))\n",
    "        c6 = self.drop6(c6)\n",
    "        c6 = F.relu(self.conv6_2(c6))\n",
    "        u7 = torch.cat((self.upconv7(c6), c3), dim=1)\n",
    "        c7 = F.relu(self.conv7_1(u7))\n",
    "        c7 = self.drop7(c7)\n",
    "        c7 = F.relu(self.conv7_2(c7))\n",
    "        u8 = torch.cat((self.upconv8(c7), c2), dim=1)\n",
    "        c8 = F.relu(self.conv8_1(u8))\n",
    "        c8 = self.drop8(c8)\n",
    "        c8 = F.relu(self.conv8_2(c8))\n",
    "        u9 = torch.cat((self.upconv9(c8), c1), dim=1)\n",
    "        c9 = F.relu(self.conv9_1(u9))\n",
    "        c9 = self.drop9(c9)\n",
    "        c9 = F.relu(self.conv9_2(c9))\n",
    "        output = torch.sigmoid(self.conv10(c9))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87530ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]  Train Loss: 0.6820  Val Loss: 0.5548\n",
      "Epoch [2/5]  Train Loss: 0.4836  Val Loss: 0.3834\n",
      "Epoch [3/5]  Train Loss: 0.3819  Val Loss: 0.3428\n",
      "Epoch [4/5]  Train Loss: 0.3503  Val Loss: 0.2971\n",
      "Epoch [5/5]  Train Loss: 0.3044  Val Loss: 0.2719\n"
     ]
    }
   ],
   "source": [
    "#training and validation loop\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = Unet()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #training\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #balance the loss per batch size\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    #validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images, masks in val_loader:\n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            #balance the loss per batch size\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76568480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean dice coefficient on test set: 0.1180\n",
      "Mean loss on test set: 0.2832\n"
     ]
    }
   ],
   "source": [
    "#testing loop\n",
    "\n",
    "def dice_coeff(outputs, masks, smooth=1e-6):\n",
    "\n",
    "    outputs = outputs.view(-1)\n",
    "    masks = masks.view(-1)\n",
    "\n",
    "    intersection = (outputs * masks).sum()\n",
    "    dice = (2. * intersection + smooth) / (outputs.sum() + masks.sum() + smooth)\n",
    "\n",
    "    return dice.item()\n",
    "\n",
    "\n",
    "test_dice = 0.0\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, masks in test_loader:\n",
    "\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        dice = dice_coeff(outputs, masks)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        #balance the loss per batch size\n",
    "        test_dice += dice * images.size(0)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "    \n",
    "test_dice /= len(test_loader.dataset)    \n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(f\"Mean dice coefficient on test set: {test_dice:.4f}\")\n",
    "print(f\"Mean loss on test set: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
